{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chassisml Example Notebooks\n",
    "Welcome to the examples section for [Chassis.ml](https://chassis.ml), which contains notebooks that leverage Chassisml to auto-containerize models built using the most common machine learning frameworks. \n",
    "\n",
    "**NOTE:** Chassisml provides two key functionalities: \n",
    "1. Create a Docker container from your model code and push that container image to a Docker registry. This is the default behavior.\n",
    "2. Should you pass valid Modzy credentials as optional parameters, Chassisml will take the container and upload it directly to the Modzy environment you specify. You will notice most of these notebooks deploy the model to one of the Modzy internal development environments.   \n",
    "\n",
    "Can't find the framework you are looking for or need help? Fork this repository and open a PR, we're always interested in growing this example bank! \n",
    "\n",
    "The primary maintainers of Chassis also actively monitor our [Discord Server](https://discord.gg/tdfXFY2y), so feel free to join and ask any questions you might have. We'll be there to respond and help out promptly. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import chassisml\n",
    "import numpy as np\n",
    "import getpass\n",
    "import json\n",
    "from io import StringIO\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn_pmml_model.ensemble import PMMLForestClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Enter credentials\n",
    "Dockerhub creds and Modzy API Key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "docker hub username········\n",
      "docker hub password········\n",
      "modzy api key········\n"
     ]
    }
   ],
   "source": [
    "dockerhub_user = getpass.getpass('docker hub username')\n",
    "dockerhub_pass = getpass.getpass('docker hub password')\n",
    "modzy_api_key = getpass.getpass('modzy api key')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load and Test Model from PMML File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare data\n",
    "iris = load_iris()\n",
    "X = pd.DataFrame(iris.data)\n",
    "X.columns = np.array(iris.feature_names)\n",
    "y = pd.Series(np.array(iris.target_names)[iris.target])\n",
    "y.name = \"Class\"\n",
    "Xtr, Xte, ytr, yte = train_test_split(X, y, test_size=0.33, random_state=123)\n",
    "\n",
    "# Create sample data for testing later\n",
    "with open(\"data/sample_iris.csv\", \"w\") as f:\n",
    "    Xte[:10].to_csv(f, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load model\n",
    "clf = PMMLForestClassifier(pmml=\"models/randomForest.pmml\")\n",
    "labels = clf.classes_.tolist()\n",
    "\n",
    "# Test model\n",
    "clf.predict(Xte)\n",
    "clf.score(Xte, yte)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare context dict\n",
    "Initialize anything here that should persist across inference runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_inputs(raw_input_bytes):\n",
    "    # load data\n",
    "    inputs = pd.read_csv(StringIO(str(raw_input_bytes, \"utf-8\")))\n",
    "    return inputs\n",
    "\n",
    "def postprocess_outputs(raw_predictions):\n",
    "    # process output\n",
    "    inference_result = {\n",
    "        \"result\":[\n",
    "            {\n",
    "                \"row\": i+1,\n",
    "                \"classPredictions\": [\n",
    "                    {\"class\": context['labels'][idx], \"score\": results[idx]}\n",
    "                    for idx in np.argsort(results)[::-1]\n",
    "                ]  \n",
    "            } for i, results in enumerate(raw_predictions)\n",
    "        ] \n",
    "\n",
    "    }    \n",
    "    \n",
    "    \n",
    "    # format output\n",
    "    structured_output = {\n",
    "        \"data\": {\n",
    "            \"result\": inference_result[\"result\"],\n",
    "            \"explanation\": None,\n",
    "            \"drift\": None,\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    return structured_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This will be passed to Chassis:\n",
    "context = {\n",
    "    \"model\": clf,\n",
    "    \"labels\": labels,\n",
    "    \"preprocess\": preprocess_inputs,\n",
    "    \"postprocess\": postprocess_outputs\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Write process function\n",
    "\n",
    "* Must take bytes and context dict as input\n",
    "* Preprocess bytes, run inference, postprocess model output, return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process(input_bytes,context):\n",
    "    # load data\n",
    "    inputs = context[\"preprocess\"](input_bytes)\n",
    "    # make predictions\n",
    "    output = context[\"model\"].predict_proba(inputs)\n",
    "    # process output\n",
    "    structured_output = context[\"postprocess\"](output)\n",
    "    return structured_output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize Chassis Client\n",
    "We'll use this to interact with the Chassis service"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "chassis_client = chassisml.ChassisClient(\"http://localhost:5000\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create and test Chassis model\n",
    "* Requires `context` dict containing all variables which should be loaded once and persist across inferences\n",
    "* Requires `process_fn` defined above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'{\"data\":{\"result\":[{\"row\":1,\"classPredictions\":[{\"class\":\"versicolor\",\"score\":0.76},{\"class\":\"virginica\",\"score\":0.24},{\"class\":\"setosa\",\"score\":0.0}]},{\"row\":2,\"classPredictions\":[{\"class\":\"virginica\",\"score\":0.995},{\"class\":\"versicolor\",\"score\":0.005},{\"class\":\"setosa\",\"score\":0.0}]},{\"row\":3,\"classPredictions\":[{\"class\":\"virginica\",\"score\":1.0},{\"class\":\"versicolor\",\"score\":0.0},{\"class\":\"setosa\",\"score\":0.0}]},{\"row\":4,\"classPredictions\":[{\"class\":\"versicolor\",\"score\":1.0},{\"class\":\"virginica\",\"score\":0.0},{\"class\":\"setosa\",\"score\":0.0}]},{\"row\":5,\"classPredictions\":[{\"class\":\"setosa\",\"score\":1.0},{\"class\":\"virginica\",\"score\":0.0},{\"class\":\"versicolor\",\"score\":0.0}]},{\"row\":6,\"classPredictions\":[{\"class\":\"virginica\",\"score\":0.82},{\"class\":\"versicolor\",\"score\":0.18},{\"class\":\"setosa\",\"score\":0.0}]},{\"row\":7,\"classPredictions\":[{\"class\":\"versicolor\",\"score\":0.995},{\"class\":\"virginica\",\"score\":0.005},{\"class\":\"setosa\",\"score\":0.0}]},{\"row\":8,\"classPredictions\":[{\"class\":\"setosa\",\"score\":1.0},{\"class\":\"virginica\",\"score\":0.0},{\"class\":\"versicolor\",\"score\":0.0}]},{\"row\":9,\"classPredictions\":[{\"class\":\"setosa\",\"score\":1.0},{\"class\":\"virginica\",\"score\":0.0},{\"class\":\"versicolor\",\"score\":0.0}]},{\"row\":10,\"classPredictions\":[{\"class\":\"versicolor\",\"score\":0.98},{\"class\":\"virginica\",\"score\":0.02},{\"class\":\"setosa\",\"score\":0.0}]}],\"explanation\":null,\"drift\":null}}'\n"
     ]
    }
   ],
   "source": [
    "# create Chassis model\n",
    "chassis_model = chassis_client.create_model(context=context,process_fn=process)\n",
    "\n",
    "# test Chassis model locally (can pass filepath, bufferedreader, bytes, or text here):\n",
    "sample_filepath = './data/sample_iris.csv'\n",
    "results = chassis_model.test(sample_filepath)\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting test job... Ok!\n",
      "{'model_output': 'Single input prediction:\\n\\nb\\'{\"data\":{\"result\":[{\"row\":1,\"classPredictions\":[{\"class\":\"versicolor\",\"score\":0.76},{\"class\":\"virginica\",\"score\":0.24},{\"class\":\"setosa\",\"score\":0.0}]},{\"row\":2,\"classPredictions\":[{\"class\":\"virginica\",\"score\":0.995},{\"class\":\"versicolor\",\"score\":0.005},{\"class\":\"setosa\",\"score\":0.0}]},{\"row\":3,\"classPredictions\":[{\"class\":\"virginica\",\"score\":1.0},{\"class\":\"versicolor\",\"score\":0.0},{\"class\":\"setosa\",\"score\":0.0}]},{\"row\":4,\"classPredictions\":[{\"class\":\"versicolor\",\"score\":1.0},{\"class\":\"virginica\",\"score\":0.0},{\"class\":\"setosa\",\"score\":0.0}]},{\"row\":5,\"classPredictions\":[{\"class\":\"setosa\",\"score\":1.0},{\"class\":\"virginica\",\"score\":0.0},{\"class\":\"versicolor\",\"score\":0.0}]},{\"row\":6,\"classPredictions\":[{\"class\":\"virginica\",\"score\":0.82},{\"class\":\"versicolor\",\"score\":0.18},{\"class\":\"setosa\",\"score\":0.0}]},{\"row\":7,\"classPredictions\":[{\"class\":\"versicolor\",\"score\":0.995},{\"class\":\"virginica\",\"score\":0.005},{\"class\":\"setosa\",\"score\":0.0}]},{\"row\":8,\"classPredictions\":[{\"class\":\"setosa\",\"score\":1.0},{\"class\":\"virginica\",\"score\":0.0},{\"class\":\"versicolor\",\"score\":0.0}]},{\"row\":9,\"classPredictions\":[{\"class\":\"setosa\",\"score\":1.0},{\"class\":\"virginica\",\"score\":0.0},{\"class\":\"versicolor\",\"score\":0.0}]},{\"row\":10,\"classPredictions\":[{\"class\":\"versicolor\",\"score\":0.98},{\"class\":\"virginica\",\"score\":0.02},{\"class\":\"setosa\",\"score\":0.0}]}],\"explanation\":null,\"drift\":null}}\\'\\n'}\n"
     ]
    }
   ],
   "source": [
    "# test environment and model within Chassis service, must pass filepath here:\n",
    "\n",
    "# dry run before build\n",
    "test_env_result = chassis_model.test_env(sample_filepath)\n",
    "print(test_env_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Publish model to Modzy\n",
    "Need to provide model name, model version, Dockerhub credentials, and required Modzy info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODZY_URL = \"https://demo.modzy.engineering/api\"\n",
    "\n",
    "response = chassis_model.publish(\n",
    "    model_name=\"PMML Random Forest Iris Classification\",\n",
    "    model_version=\"0.0.1\",\n",
    "    registry_user=dockerhub_user,\n",
    "    registry_pass=dockerhub_pass,\n",
    "    modzy_sample_input_path=sample_filepath,\n",
    "    modzy_api_key=modzy_api_key,\n",
    "    modzy_url=MODZY_URL\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "job_id = response.get('job_id')\n",
    "final_status = chassis_client.block_until_complete(job_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New model URL: https://demo.modzy.engineering/models/ocljmj1dvy/0.0.1\n"
     ]
    }
   ],
   "source": [
    "if final_status[\"result\"] is not None:\n",
    "    print(\"New model URL: {}\".format(final_status[\"result\"][\"container_url\"]))\n",
    "else:\n",
    "    print(\"Chassis job failed \\n\\n {}\".format(final_status))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run sample job using Modzy SDK\n",
    "Submit inference job to our newly-deploy model running on Modzy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from modzy import ApiClient\n",
    "\n",
    "client = ApiClient(base_url=MODZY_URL, api_key=modzy_api_key)\n",
    "\n",
    "input_name = final_status['result']['inputs'][0]['name']\n",
    "model_id = final_status['result'].get(\"model\").get(\"modelId\")\n",
    "model_version = final_status['result'].get(\"version\")\n",
    "\n",
    "inference_job = client.jobs.submit_file(model_id, model_version, {input_name: sample_filepath})\n",
    "inference_job_result = client.results.block_until_complete(inference_job, timeout=None)\n",
    "inference_job_results_json = inference_job_result.get_first_outputs()['results.json']\n",
    "print(inference_job_results_json)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "95ab8d3bdd708a5b71676285742277315a30b0df1ed91c8905076616709c59d5"
  },
  "kernelspec": {
   "display_name": "chassis-demo-1",
   "language": "python",
   "name": "chassis-demo-1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
